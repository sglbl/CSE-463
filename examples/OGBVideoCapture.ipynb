{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\batch_distance.cpp:275: error: (-215:Assertion failed) type == src2.type() && src1.cols == src2.cols && (type == CV_32F || type == CV_8U) in function 'cv::batchDistance'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Initialize the Matcher for matching the keypoints and then match the keypoints \u001b[39;00m\n\u001b[0;32m     48\u001b[0m matcher \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mBFMatcher(cv2\u001b[38;5;241m.\u001b[39mNORM_HAMMING, crossCheck\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 49\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[43mmatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueryDescriptors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrainDescriptors\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Sort/re-arrange them by distance measured lambda call\u001b[39;00m\n\u001b[0;32m     52\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(matches, key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x:x\u001b[38;5;241m.\u001b[39mdistance)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\batch_distance.cpp:275: error: (-215:Assertion failed) type == src2.type() && src1.cols == src2.cols && (type == CV_32F || type == CV_8U) in function 'cv::batchDistance'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Real-Time Edge Detection using OpenCV in Python | Canny edge detection method\n",
    "+ cv2.ORB_create() addition to display points of interest over video stream.\n",
    "Welcome to a feature matching tutorial with OpenCV and Python. \n",
    "Feature matching is going to be a slightly more impressive version of template matching,\n",
    "where a perfect, or very close to perfect, match is required.\n",
    "\n",
    "We start with the image that we're hoping to find, and then we can search for this image within another image\n",
    "produced by a camera stream. The beauty here is that the image does not need to be the same lighting, angle, rotation...etc. \n",
    "The features just need to match up.\n",
    "author: Peter Mankowski\n",
    "May 2020\n",
    "\"\"\"\n",
    "\n",
    "import cv2  \n",
    "import numpy as np \n",
    "\n",
    "# variable init calls\n",
    "match_number = 50\n",
    "  \n",
    "# capture frames from a camera \n",
    "cap = cv2.VideoCapture(0) \n",
    "  \n",
    "while(1):   \n",
    "    # reads frames from a camera \n",
    "    ret, frame = cap.read() \n",
    "    \n",
    "    # Place a 'train.jpg' of the image we will be matching\n",
    "    query_img = cv2.imread('flowers-left.png')  \n",
    "\n",
    "    # Convert both streams to grayscale \n",
    "    query_img_bw = cv2.cvtColor(query_img, cv2.COLOR_BGR2GRAY) # Picture \n",
    "    train_img_bw = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)     # Streaming video sample \n",
    "    \n",
    "    # This is an optional part of dumping both on the screen for debugging\n",
    "    cv2.imshow(\"query_img_bw\", query_img_bw)\n",
    "    cv2.imshow(\"train_img_bw\", train_img_bw)\n",
    "    \n",
    "    # Initialize the ORB detector algorithm \n",
    "    orb = cv2.ORB_create(nfeatures=500, scaleFactor=1.2, nlevels=8, edgeThreshold=31, \n",
    "                         firstLevel=0, WTA_K=2, patchSize=31, fastThreshold=20) \n",
    "\n",
    "    # Now detect the keypoints and compute the descriptors for the query image and train image \n",
    "    queryKeypoints, queryDescriptors = orb.detectAndCompute(query_img_bw,None) \n",
    "    trainKeypoints, trainDescriptors = orb.detectAndCompute(train_img_bw,None) \n",
    "\n",
    "    # Initialize the Matcher for matching the keypoints and then match the keypoints \n",
    "    matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = matcher.match(queryDescriptors,trainDescriptors) \n",
    "    \n",
    "    # Sort/re-arrange them by distance measured lambda call\n",
    "    matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "    # draw all matches to the final image containing both the images the drawMatches() \n",
    "    # function takes both images and keypoints and outputs the matched query image with its train image \n",
    "    final_img = cv2.drawMatches(query_img, queryKeypoints,  \n",
    "    frame, trainKeypoints, matches[:match_number],None) \n",
    "\n",
    "    final_img = cv2.resize(final_img, (1000,650)) \n",
    "\n",
    "    # Show the final image of the ORB feature \n",
    "    cv2.imshow(\"ORB Matches nodes\", final_img)      \n",
    "  \n",
    "    # Edge Detector portion\n",
    "    # converting BGR to HSV \n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) \n",
    "      \n",
    "    # define range of red color in HSV \n",
    "    lower_red = np.array([30,150,50]) \n",
    "    upper_red = np.array([255,255,180]) \n",
    "      \n",
    "    # create a red HSV colour boundary and  \n",
    "    # threshold HSV image \n",
    "    mask = cv2.inRange(hsv, lower_red, upper_red) \n",
    "  \n",
    "    # Bitwise-AND mask and original image \n",
    "    res = cv2.bitwise_and(frame,frame, mask= mask) \n",
    "  \n",
    "    # Display an original image \n",
    "    cv2.imshow('Original',frame) \n",
    "  \n",
    "    # finds edges in the input image image and \n",
    "    # marks them in the output map edges \n",
    "    edges = cv2.Canny(frame,100,200) \n",
    "  \n",
    "    # Display edges in a frame \n",
    "    cv2.imshow('Edges',edges) \n",
    "  \n",
    "    # Wait for Esc key to stop \n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27: \n",
    "        break \n",
    "  \n",
    "# Close the window \n",
    "cap.release() \n",
    "  \n",
    "# De-allocate any associated memory usage \n",
    "cv2.destroyAllWindows()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
