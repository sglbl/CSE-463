{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad2254dd",
   "metadata": {},
   "source": [
    "SIFT OBJECT RECOGNIZING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e771bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait while the data is being loaded (it may take 1-2 minutes)...\n",
      "Total images checked  111\n",
      "Total descriptor sets:  111\n",
      "Total descriptors with respect to labels added:  6046\n",
      "New shape of Descriptors:  (6046, 128)\n",
      "KMEANS CLUSTERING (it may take 10-15 minutes for big data, 0-1 minute for small data)...\n",
      "clusteringLabels.shape : (6046,)\n",
      "len(labelsOfData) : 6046  and len(labelsOfClustering) :  6046\n",
      "Accuracy of classicifaction:  0.6366192523982799\n",
      "Confusion Matrix: \n",
      "             banana  calculator  camera  cell_phone  flashlight  food_bag  \\\n",
      "banana          86          10      23           0          30        28   \n",
      "calculator       6        1005       5          12          79       229   \n",
      "camera           2          35      67           4          35       128   \n",
      "cell_phone       0          51      14          63          20        37   \n",
      "flashlight      17          52      41           1         288       168   \n",
      "food_bag        32         189      42          38          98      2079   \n",
      "lemon           10          16       0           0          17        13   \n",
      "lightbulb        8          16       0          11          16        99   \n",
      "lime             0           2       6           1          35        31   \n",
      "marker          16          23      13           0          25        48   \n",
      "\n",
      "            lemon  lightbulb  lime  marker  \n",
      "banana          6          0    15      18  \n",
      "calculator     13          0    17      11  \n",
      "camera          0          0    14      13  \n",
      "cell_phone      3          0     6       3  \n",
      "flashlight     19          3    52       5  \n",
      "food_bag        8          5    22      27  \n",
      "lemon          58          0    28      17  \n",
      "lightbulb       5         10     3       5  \n",
      "lime            1          0    83       4  \n",
      "marker          9          9    24     110  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.cluster\n",
    "import sklearn.metrics\n",
    "import sklearn.preprocessing\n",
    "\n",
    "class RecognizerSift:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.trainingSet = []\n",
    "        self.testSet = []\n",
    "        nameOfClasses = ['banana_1', 'calculator_1', 'camera_1', 'cell_phone_1', 'flashlight_1', 'food_bag_1', 'lemon_1', 'lightbulb_1', 'lime_1', 'marker_1']\n",
    "        nameLabelsOfClasses = {nameOfClass:i for i, nameOfClass in enumerate(nameOfClasses)}\n",
    "\n",
    "        numberOfClasses = len(nameOfClasses)\n",
    "        self.dataLoader(nameLabelsOfClasses)\n",
    "\n",
    "    def siftDetectsAndComputes(self, image):\n",
    "        sift = cv.SIFT_create()\n",
    "        grayscaleImage1 = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "        keyPoints1, descriptors1 = sift.detectAndCompute(grayscaleImage1, None)\n",
    "        image = cv.drawKeypoints(grayscaleImage1, keyPoints1, image)\n",
    "        return image, descriptors1\n",
    "\n",
    "    def dataLoader(self, nameLabelsOfClasses):\n",
    "        datasetDirectory = os.path.join(os.getcwd(), \"dataset\")\n",
    "        objectIndex = 0\n",
    "        setOfDescriptors = [] \n",
    "        setOfImages = []\n",
    "        labelsOfData = []\n",
    "\n",
    "        print(\"Please wait while the data is being loaded (it may take 1-2 minutes)...\")\n",
    "        # Choose random 90% of the objects to be the training set in every class folder\n",
    "        for imageFolder in os.listdir(datasetDirectory):\n",
    "            # label = nameLabelsOfClasses[imageFolder]\n",
    "            label = objectIndex\n",
    "            objectIndex += 1\n",
    "    \n",
    "            imageFolderPath = os.path.join(datasetDirectory, imageFolder)\n",
    "\n",
    "            ''''''''''''''' TRAINING PART '''''''''''''''\n",
    "            trainingClassFolderList = os.listdir(imageFolderPath)\n",
    "            # choose random 90& of the objects and get their path\n",
    "            trainingClassFolderList = np.random.choice(trainingClassFolderList, int(len(trainingClassFolderList)*0.9), replace=False)\n",
    "            trainingClassFolderList = [os.path.join(imageFolderPath, x) for x in trainingClassFolderList]\n",
    "\n",
    "            # select the png files that doesn't end with depthcrop.png and maskcrop.png and get their path\n",
    "            trainingClassFolderList = [x for x in trainingClassFolderList if os.path.isfile(x) \n",
    "                        and x.endswith(\".png\") and not x.endswith(\"maskcrop.png\") and not x.endswith(\"depthcrop.png\") ]         \n",
    "\n",
    "            # sorting training class folder list\n",
    "            sortedFiles = sorted(trainingClassFolderList)\n",
    "            # print(\"Sortedf files are: \" , sortedFiles)\n",
    "            for file in sortedFiles:\n",
    "                imageWithKeyPoints, descriptors = self.siftDetectsAndComputes(cv.imread(file))\n",
    "                setOfImages.append(imageWithKeyPoints)\n",
    "                setOfDescriptors.append(descriptors)\n",
    "\n",
    "                for descIndex in range(0, len(descriptors)):\n",
    "                    labelsOfData.append(label)\n",
    "\n",
    "    \n",
    "        # print(\"Labels of data are: \", labelsOfData) # so big array of labels so if want to print be careful\n",
    "        print(\"Total images checked \", len(setOfImages))\n",
    "        print(\"Total descriptor sets: \", len(setOfDescriptors))\n",
    "        print(\"Total descriptors with respect to labels added: \", len(labelsOfData))\n",
    "\n",
    "        # concatanate all the descriptors into one array for training\n",
    "        descriptors = np.vstack(setOfDescriptors)\n",
    "        print(\"New shape of Descriptors: \", np.shape(descriptors))\n",
    "        \n",
    "        newDescriptorData = descriptors\n",
    "        # PART OF K-MEANS CLUSTERING TO FIND THE GROUP OF DESCRIPTORS THAT BELONG TO THE SAME CLASS\n",
    "        print(\"KMEANS CLUSTERING (it may take 1 minute)...\")\n",
    "        # First, normalize the data to the range [0,1] to make the clustering more robust to different lighting conditions.\n",
    "        scalerObject = sklearn.preprocessing.MinMaxScaler()\n",
    "        newDescriptorData = scalerObject.fit_transform(descriptors)\n",
    "\n",
    "        # Apply k-means clustering to the descriptors to obtain the training set\n",
    "        kmeans = sklearn.cluster.KMeans(n_clusters=256)\n",
    "        kmeans.fit(newDescriptorData)\n",
    "\n",
    "        # Checking the size of labels to obtain a label for each of the descriptors\n",
    "        labelsOfClustering = kmeans.predict(newDescriptorData)\n",
    "        print(\"clusteringLabels.shape :\" , np.shape(labelsOfClustering))\n",
    "\n",
    "        # Clustering labels are now the labels of the training set which associated with it\n",
    "        # For example second index of clusterinLabels is\n",
    "        # print(\"clusteringLabels[2] :\" , labelsOfClustering[2])\n",
    "        # Cluster label and the label of the data are the same (printing)\n",
    "        print(\"len(labelsOfData) :\" , len(labelsOfData), \" and len(labelsOfClustering) : \", len(labelsOfClustering))\n",
    "\n",
    "        predictedLabels = []\n",
    "        predictedLabels = self.getLabelArrayOfClusters(labelsOfClustering, labelsOfData)\n",
    "\n",
    "        print(\"Accuracy of classicifaction: \", sklearn.metrics.accuracy_score(labelsOfData, predictedLabels))\n",
    "\n",
    "        rows = [\"banana\", \"calculator\", \"camera\", \"cell_phone\", \"flashlight\", \"food_bag\", \"lemon\", \"lightbulb\", \"lime\", \"marker\"]\n",
    "        columns = [\"banana\", \"calculator\", \"camera\", \"cell_phone\", \"flashlight\", \"food_bag\", \"lemon\", \"lightbulb\", \"lime\", \"marker\"]\n",
    "        confusionMatrix = sklearn.metrics.confusion_matrix(labelsOfData, predictedLabels, labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "        matrixTable = pd.DataFrame(confusionMatrix, rows, columns)\n",
    "        print(\"Confusion Matrix: \\n\", matrixTable)\n",
    "\n",
    "            # # labelsTraining.append(label)\n",
    "            \n",
    "            # ''''''''''''''' TESTING PART '''''''''''''''\n",
    "            # # choose other 10% of the objects and get their path   \n",
    "            # testingFolderList = [x for x in trainingClassFolderList if x not in trainingClassFolderList[:int(len(trainingClassFolderList)*0.9)]]\n",
    "            # # select the png files that doesn't end with depthcrop.png and maskcrop.png and get their path\n",
    "            # testingFolderList = [x for x in testingFolderList  if os.path.isfile(x) \n",
    "            #             and x.endswith(\".png\") and not x.endswith(\"depthcrop.png\") and not x.endswith(\"maskcrop.png\")]\n",
    "\n",
    "        #     # read the images and resize them\n",
    "        #     testingImages = [cv.imread(x) for x in testingFolderList]\n",
    "        #     # convert every image to rgb\n",
    "        #     testingImages = [cv.cvtColor(x, cv.COLOR_BGR2RGB) for x in testingImages]\n",
    "        #     # resize every image to IMAGE_SIZE which is (100,100) to speed up the training\n",
    "        #     testingImages = [cv.resize(x, IMAGE_SIZE) for x in testingImages]\n",
    "\n",
    "        #     labelsTest.append(label)\n",
    "        #     # imagesTestingAll.append(testingImages)\n",
    "        #     imagesTestingAll = testingImages\n",
    "        #     # End of for loop for every class folder\n",
    "            \n",
    "        # # convert the lists to numpy arrays\n",
    "        # imagesTrainingAll = np.array(imagesTrainingAll, dtype=np.float32)\n",
    "        # imagesTestingAll = np.array(imagesTestingAll, dtype=np.float32)\n",
    "        # labelsTraining = np.array(labelsTraining, dtype=np.int32)\n",
    "        # labelsTest = np.array(labelsTest, dtype=np.int32)\n",
    "\n",
    "        # # Append the training and testing sets and their labels\n",
    "        # output.append( (imagesTrainingAll, labelsTraining) )\n",
    "        # output.append( (imagesTestingAll, labelsTest) )\n",
    "\n",
    "        # return output    \n",
    "        \n",
    "    def getLabelArrayOfClusters(self, labelsOfClustering, labelsOfData):\n",
    "        indexMap = {}\n",
    "\n",
    "        for i in range( len(np.unique(labelsOfClustering)) ):\n",
    "            index = np.where(labelsOfClustering == i,1,0)\n",
    "\n",
    "            allIndices = [i2 for i2, x in enumerate(index) if x == 1]\n",
    "            # _ = []\n",
    "            labelSaver = []\n",
    "            for jindex in allIndices:\n",
    "                labelSaver.append(labelsOfData[jindex])\n",
    "\n",
    "\n",
    "            # counting the most common label in the cluster \n",
    "            # bincount => counts the number of occurrences of each value in an array\n",
    "            # argmax => returns the index of the largest value in an array (if multiple return index array)\n",
    "            number = np.bincount(labelSaver).argmax()\n",
    "\n",
    "            indexMap[i] = number\n",
    "        # now we have the index map of the labels of the clusters which are the labels of the training set\n",
    "\n",
    "        clusterLabelsLength = len(labelsOfClustering)\n",
    "        predictedLabels = np.random.rand( clusterLabelsLength )\n",
    "        for i in range(clusterLabelsLength):\n",
    "            predictedLabels[i] = indexMap[labelsOfClustering[i]]\n",
    "        return predictedLabels\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    RecognizerSift()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048e30aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
